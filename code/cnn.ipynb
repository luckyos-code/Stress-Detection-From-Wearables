{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import scipy.signal\n",
    "from scipy import fft\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.realpath(\"../data/WESAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject:\n",
    "    \"\"\"Subject of the WESAD dataset.\n",
    "    Subject Class inspired by: https://github.com/WJMatthew/WESAD\"\"\"\n",
    "\n",
    "    def __init__(self, main_path, subject_number):\n",
    "        self.name = f'S{subject_number}'\n",
    "        self.subject_keys = ['signal', 'label', 'subject']\n",
    "        self.signal_keys = ['chest', 'wrist']\n",
    "        self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "        self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "            self.data = pickle.load(file, encoding='latin1')\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        \"\"\"Returns data measured by the E4 Empatica\"\"\"\n",
    "\n",
    "        data = self.data['signal']['wrist']\n",
    "        return data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        \"\"\"Returns data measured by the Respiband wristband\"\"\"\n",
    "        return self.data['signal']['chest']\n",
    "    \n",
    "    def get_subject_dataframe(self):\n",
    "        \"\"\"Returns a dataframe with the preprocessed data of the subject\"\"\"\n",
    "        wrist_data = self.get_wrist_data()\n",
    "        bvp_signal = wrist_data['BVP'][:,0]\n",
    "        eda_signal = wrist_data['EDA'][:,0]\n",
    "        acc_x_signal = wrist_data['ACC'][:,0]\n",
    "        acc_y_signal = wrist_data['ACC'][:,1]\n",
    "        acc_z_signal = wrist_data['ACC'][:,2]\n",
    "        temp_signal = wrist_data['TEMP'][:,0]\n",
    "        # Upsampling data to match BVP data sampling rate using fourier method as described in Paper/dataset\n",
    "        eda_upsampled = scipy.signal.resample(eda_signal, len(bvp_signal))\n",
    "        temp_upsampled = scipy.signal.resample(temp_signal, len(bvp_signal))\n",
    "        acc_x_upsampled = scipy.signal.resample(acc_x_signal, len(bvp_signal))\n",
    "        acc_y_upsampled = scipy.signal.resample(acc_y_signal, len(bvp_signal))\n",
    "        acc_z_upsampled = scipy.signal.resample(acc_z_signal, len(bvp_signal))\n",
    "        label_df = pd.DataFrame(self.labels, columns=['label'])\n",
    "        label_df.index = [(1 / 700) * i for i in range(len(label_df))] # 700 is the sampling rate of the label\n",
    "        label_df.index = pd.to_datetime(label_df.index, unit='s')\n",
    "        data_arrays = zip(bvp_signal, eda_upsampled, acc_x_upsampled, acc_y_upsampled, acc_z_upsampled, temp_upsampled)\n",
    "        df = pd.DataFrame(data=data_arrays, columns=['BVP', 'EDA', 'ACC_x', 'ACC_y', 'ACC_z', 'TEMP'])\n",
    "        df.index = [(1 / 64) * i for i in range(len(df))] # 64 = sampling rate of BVP\n",
    "        df.index = pd.to_datetime(df.index, unit='s')\n",
    "        df = df.join(label_df)\n",
    "        df['label'] = df['label'].fillna(method='ffill')\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.drop(df[df['label'].isin([0.0, 4.0, 5.0, 6.0, 7.0])].index, inplace=True)\n",
    "        df['label'] = df['label'].replace([1.0, 2.0, 3.0], [0, 1, 0])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df = (df-df.min())/(df.max()-df.min()) # Normalize data (no train test leakage since data frame per subject)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with all the subjects and belonging dataframes\n",
    "subjects = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "subjects_data = {}\n",
    "for subject_num in subjects:\n",
    "    subject = Subject(DATA_PATH, subject_num)\n",
    "    subjects_data[subject.name] = subject.get_subject_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subwindow length of the biosignals\n",
    "signal_subwindow_dict = {\n",
    "    'ACC_x': 7,\n",
    "    'ACC_y': 7,\n",
    "    'ACC_z': 7,\n",
    "    'BVP': 30,\n",
    "    'EDA': 30,\n",
    "    'TEMP': 35\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent element in list\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(df: pd.DataFrame) -> tuple[pd.DataFrame,list]:\n",
    "    \"\"\"Creates windows from the dataframe and returns the windows and the labels.\n",
    "    If the window is assigned to multiple labels, the most common label is chosen for that period.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Subject DataFrame\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame,list]: Windows representing the activity of the subject in one minute and the corresponding labels.\n",
    "    \"\"\"\n",
    "\n",
    "    window_len = 64 * 60 # fs = 64 and window length in seconds = 60\n",
    "    windows, labels = zip(*[(df[i:i+window_len], int(most_common(df['label'][i:i+window_len].to_list()))) for i in range(0,df.shape[0],window_len)])\n",
    "    return windows, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subwindows(df: pd.DataFrame, signal_subwindow_len: int, signal_name: str) -> list:\n",
    "    \"\"\"The function creates subwindows from the windows.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Windows representing the activity of the subject in one minute.\n",
    "        signal_subwindow_len (int): Length of the subwindows.\n",
    "        signal_name (str): Name of the signal.\n",
    "\n",
    "    Returns:\n",
    "        list: Subwindows of the signal in the window.\n",
    "    \"\"\"\n",
    "    subwindow_len = 64 * signal_subwindow_len # fs = 64 and sub-window length in seconds = 30\n",
    "    window_len = 64 * 60 # fs = 64 and window length in seconds = 60\n",
    "    window_shift = int(64 * 0.25) # fs = 64 and window shift in seconds = 0.25\n",
    "    subwindows = []\n",
    "\n",
    "    for i in range(0, window_len, window_shift):\n",
    "        if i + subwindow_len <= window_len:\n",
    "            subwindow = df[signal_name][i:i+subwindow_len]\n",
    "            subwindows.append(subwindow)\n",
    "    return subwindows\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_subwindows(subwindows: list, duration: int, f_s: int) -> list:\n",
    "    \"\"\"Calculates the fft of the subwindows.\n",
    "\n",
    "    Args:\n",
    "        subwindows (list): C\n",
    "        duration (int): _description_\n",
    "        f_s (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        list: Fft coefficients of the subwindows.\n",
    "    \"\"\"\n",
    "    freqs= []\n",
    "    yfs = []\n",
    "    for subwindow in subwindows:\n",
    "        y = np.array(subwindow)\n",
    "        yf = scipy.fft.fft(y)\n",
    "        l = len(yf)\n",
    "        N = f_s * duration\n",
    "        freq = scipy.fft.fftfreq(N, 1/f_s)\n",
    "\n",
    "        l //= 2\n",
    "        amps = np.abs(yf[0:l])\n",
    "        freq = np.abs(freq[0:l])\n",
    "\n",
    "        # Sort descending amp   \n",
    "        p = amps.argsort()[::-1]\n",
    "        freq = freq[p]\n",
    "        amps = amps[p]\n",
    "\n",
    "        freqs.append(freq)\n",
    "        yfs.append(amps)\n",
    "    return np.asarray(freqs), np.asarray(yfs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_window(subwindows_fft: list) -> list:\n",
    "    \"\"\"Calculates the average of the fft coefficients of the subwindows.\n",
    "\n",
    "    Args:\n",
    "        subwindows_fft (list): List of fft coefficients of the subwindows.\n",
    "\n",
    "    Returns:\n",
    "        list: Average of the fft coefficients of the subwindow for signals.\n",
    "    \"\"\"\n",
    "    len_yfs = len(subwindows_fft[0])\n",
    "    avg_yfs = []\n",
    "    for i in range(len_yfs):\n",
    "        i_yfs = []\n",
    "        for yf in subwindows_fft:\n",
    "            try:\n",
    "                i_yfs.append(yf[i])\n",
    "            except IndexError:\n",
    "                pass\n",
    "        avg_yfs.append(sum(i_yfs)/len(i_yfs))\n",
    "    return avg_yfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates averaged windows for all subjects from dataframes\n",
    "\n",
    "subjects_preprosessed_data = {}\n",
    "for subject_name, subject_df in subjects_data.items():\n",
    "    subjects_preprosessed_data[subject_name] = {}\n",
    "    windows, labels = create_windows(subject_df)\n",
    "    yfs_per_min_for_signal = {}\n",
    "    X = []\n",
    "    for i in range(0,len(windows) - 1):\n",
    "        for signal in signal_subwindow_dict.keys():\n",
    "\n",
    "            duration_in_sec = signal_subwindow_dict[signal]\n",
    "\n",
    "            subwindows = create_subwindows(windows[i], signal_subwindow_len=duration_in_sec, signal_name=signal)\n",
    "            freqs, yfs = fft_subwindows(subwindows, duration_in_sec, 64)\n",
    "            yfs_average = average_window(yfs)[:210]\n",
    "            yfs_per_min_for_signal[signal] = yfs_average\n",
    "            \n",
    "        X.append(pd.DataFrame(yfs_per_min_for_signal).T)\n",
    "    y = list(labels[:len(windows)-1])\n",
    "    subjects_preprosessed_data[subject_name]['X'] = X\n",
    "    subjects_preprosessed_data[subject_name]['y'] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created train and test data for leave one out cross validation\n",
    "all_subjects_X = []\n",
    "all_subjects_y = []\n",
    "for subject_name, subject_data in subjects_preprosessed_data.items():\n",
    "    all_subjects_X.append(subject_data['X'])\n",
    "    all_subjects_y.append(subject_data['y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_signals: int, num_output_class: int) -> tf.keras.models.Sequential:\n",
    "    # Define the model architecture\n",
    "    model = tf.keras.Sequential()\n",
    "    # input_shape = 14 Signale (bei uns max. 6) X 210 Inputs (aus Tabelle nach Fourier)\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[num_signals, 210, 1]))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, activation='relu', kernel_size=(1,3), strides=1, padding='same')) \n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, activation='relu', kernel_size=(1,3), strides=1, padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(1,2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, activation='relu', kernel_size=(1,3), strides=1, padding='same'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(1,2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=128, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Dense(units=64, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    # Anzahl der Units = Anzahl der Klassen (2 - non-stress vs stress)\n",
    "    model.add(tf.keras.layers.Dense(units=num_output_class, activation='sigmoid', kernel_initializer='glorot_uniform')) # sigmoid statt softmax, da nur 2 Klassen\n",
    "\n",
    "    model.compile(optimizer=\"rmsprop\",loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/2h0x6gds2qg6pxyf4w18xmcr0000gn/T/ipykernel_2665/3640312202.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train = np.concatenate(np.array([all_subjects_X[x] for x in train_index]))\n",
      "/var/folders/0x/2h0x6gds2qg6pxyf4w18xmcr0000gn/T/ipykernel_2665/3640312202.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y_train = np.concatenate(np.array([all_subjects_y[y] for y in train_index]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 8s 423ms/step - loss: 5.2781 - accuracy: 0.6392\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 448ms/step - loss: 1.0270 - accuracy: 0.7784\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 459ms/step - loss: 0.8590 - accuracy: 0.8098\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 440ms/step - loss: 0.7770 - accuracy: 0.8412\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s 489ms/step - loss: 0.6776 - accuracy: 0.8216\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 440ms/step - loss: 0.6792 - accuracy: 0.8176\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 440ms/step - loss: 0.6302 - accuracy: 0.8627\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 446ms/step - loss: 0.6235 - accuracy: 0.8431\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 470ms/step - loss: 0.6011 - accuracy: 0.8569\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s 462ms/step - loss: 0.5952 - accuracy: 0.8569\n",
      "[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] 1\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 6s 455ms/step - loss: 4.3831 - accuracy: 0.6922\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 476ms/step - loss: 1.0900 - accuracy: 0.7863\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 474ms/step - loss: 0.9017 - accuracy: 0.8196\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 476ms/step - loss: 0.7320 - accuracy: 0.8647\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s 474ms/step - loss: 0.6512 - accuracy: 0.8431\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 482ms/step - loss: 0.6317 - accuracy: 0.8627\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 452ms/step - loss: 0.6859 - accuracy: 0.8471\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 476ms/step - loss: 0.6552 - accuracy: 0.8725\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 490ms/step - loss: 0.5534 - accuracy: 0.8549\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s 451ms/step - loss: 0.5205 - accuracy: 0.8824\n",
      "[0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] 2\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 7s 460ms/step - loss: 3.9107 - accuracy: 0.6699\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 493ms/step - loss: 1.0564 - accuracy: 0.7819\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 471ms/step - loss: 0.8068 - accuracy: 0.8075\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 474ms/step - loss: 0.7222 - accuracy: 0.8212\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s 470ms/step - loss: 0.7150 - accuracy: 0.8193\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 459ms/step - loss: 0.6914 - accuracy: 0.8153\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 406ms/step - loss: 0.6489 - accuracy: 0.8448\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 4s 396ms/step - loss: 0.5755 - accuracy: 0.8684\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 111s 11s/step - loss: 0.5856 - accuracy: 0.8605\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 6s 535ms/step - loss: 0.6050 - accuracy: 0.8605\n",
      "[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] 3\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 6s 354ms/step - loss: 5.2176 - accuracy: 0.6758\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 418ms/step - loss: 1.0067 - accuracy: 0.7839\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 446ms/step - loss: 0.8580 - accuracy: 0.7996\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 441ms/step - loss: 0.7909 - accuracy: 0.8212\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s 478ms/step - loss: 0.7424 - accuracy: 0.8350\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 496ms/step - loss: 0.6457 - accuracy: 0.8507\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 455ms/step - loss: 0.5643 - accuracy: 0.8743\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 462ms/step - loss: 0.6551 - accuracy: 0.8664\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 6s 528ms/step - loss: 0.5770 - accuracy: 0.8546\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s 448ms/step - loss: 0.5444 - accuracy: 0.8684\n",
      "[0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] 4\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 6s 470ms/step - loss: 4.2647 - accuracy: 0.6876\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 477ms/step - loss: 1.0967 - accuracy: 0.7780\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 436ms/step - loss: 0.7722 - accuracy: 0.8271\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 461ms/step - loss: 0.8098 - accuracy: 0.8330\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s 492ms/step - loss: 0.6455 - accuracy: 0.8664\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 466ms/step - loss: 0.6217 - accuracy: 0.8566\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 452ms/step - loss: 0.7094 - accuracy: 0.8251\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 482ms/step - loss: 0.5396 - accuracy: 0.8487\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 470ms/step - loss: 0.5977 - accuracy: 0.8762\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s 448ms/step - loss: 0.5582 - accuracy: 0.8723\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x130d35ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14] 5\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 6s 433ms/step - loss: 4.4944 - accuracy: 0.6680\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 466ms/step - loss: 1.0470 - accuracy: 0.7878\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 463ms/step - loss: 0.9709 - accuracy: 0.8035\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 432ms/step - loss: 0.8179 - accuracy: 0.8114\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 6s 551ms/step - loss: 0.7514 - accuracy: 0.8468\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 464ms/step - loss: 0.5832 - accuracy: 0.8251\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 4s 316ms/step - loss: 0.6515 - accuracy: 0.8546\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 3s 309ms/step - loss: 0.6627 - accuracy: 0.8507\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 4s 336ms/step - loss: 0.6426 - accuracy: 0.8369\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 4s 338ms/step - loss: 0.5742 - accuracy: 0.8428\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x130fb61f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14] 6\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 48s 631ms/step - loss: 3.6651 - accuracy: 0.6857\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 1.0497 - accuracy: 0.7957\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 6s 582ms/step - loss: 0.7214 - accuracy: 0.8448\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 6s 498ms/step - loss: 0.8293 - accuracy: 0.8193\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s 449ms/step - loss: 0.7689 - accuracy: 0.8369\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 450ms/step - loss: 0.6694 - accuracy: 0.8448\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 457ms/step - loss: 0.5734 - accuracy: 0.8841\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 6s 564ms/step - loss: 0.6299 - accuracy: 0.8507\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 442ms/step - loss: 0.6564 - accuracy: 0.8585\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 6s 536ms/step - loss: 0.5692 - accuracy: 0.8782\n",
      "[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14] 7\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 7s 452ms/step - loss: 4.1381 - accuracy: 0.6837\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 439ms/step - loss: 1.0702 - accuracy: 0.7741\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 429ms/step - loss: 0.9298 - accuracy: 0.7741\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 464ms/step - loss: 0.6778 - accuracy: 0.8310\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s 489ms/step - loss: 0.6968 - accuracy: 0.8369\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 463ms/step - loss: 0.7587 - accuracy: 0.8173\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 471ms/step - loss: 0.6671 - accuracy: 0.8114\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 466ms/step - loss: 0.6502 - accuracy: 0.8330\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 452ms/step - loss: 0.6762 - accuracy: 0.8487\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s 456ms/step - loss: 0.5188 - accuracy: 0.8605\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14] 8\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 7s 470ms/step - loss: 5.4068 - accuracy: 0.6555\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 485ms/step - loss: 1.1042 - accuracy: 0.7323\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 463ms/step - loss: 0.8153 - accuracy: 0.7992\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 428ms/step - loss: 0.8036 - accuracy: 0.8209\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 7s 667ms/step - loss: 0.7692 - accuracy: 0.8150\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 7s 578ms/step - loss: 0.6144 - accuracy: 0.8209\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 492ms/step - loss: 0.5864 - accuracy: 0.8425\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 6s 519ms/step - loss: 0.6915 - accuracy: 0.8740\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 446ms/step - loss: 0.5976 - accuracy: 0.8425\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s 448ms/step - loss: 0.6301 - accuracy: 0.8484\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14] 9\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 7s 474ms/step - loss: 4.4263 - accuracy: 0.6555\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 490ms/step - loss: 0.9985 - accuracy: 0.7835\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 470ms/step - loss: 0.7639 - accuracy: 0.8307\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 6s 556ms/step - loss: 0.7442 - accuracy: 0.8169\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 6s 575ms/step - loss: 0.6833 - accuracy: 0.8287\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 6s 558ms/step - loss: 0.5651 - accuracy: 0.8504\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 6s 481ms/step - loss: 0.6022 - accuracy: 0.8602\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 479ms/step - loss: 0.6443 - accuracy: 0.8445\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 6s 512ms/step - loss: 0.5027 - accuracy: 0.8661\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 6s 561ms/step - loss: 0.5658 - accuracy: 0.8701\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14] 10\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 7s 543ms/step - loss: 3.1233 - accuracy: 0.7303\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 445ms/step - loss: 0.9129 - accuracy: 0.8110\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 4s 383ms/step - loss: 0.8466 - accuracy: 0.8130\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 407ms/step - loss: 0.7059 - accuracy: 0.8346\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 4s 386ms/step - loss: 0.7683 - accuracy: 0.8327\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 443ms/step - loss: 0.5946 - accuracy: 0.8268\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 419ms/step - loss: 0.6900 - accuracy: 0.8071\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 416ms/step - loss: 0.5930 - accuracy: 0.8445\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 457ms/step - loss: 0.5363 - accuracy: 0.8681\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 6s 552ms/step - loss: 0.5599 - accuracy: 0.8701\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14] 11\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 7s 449ms/step - loss: 5.7885 - accuracy: 0.6969\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 423ms/step - loss: 0.6713 - accuracy: 0.8465\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 4s 396ms/step - loss: 0.6709 - accuracy: 0.8484\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 4s 388ms/step - loss: 0.6825 - accuracy: 0.8504\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 4s 374ms/step - loss: 0.5797 - accuracy: 0.8602\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 4s 391ms/step - loss: 0.5996 - accuracy: 0.8819\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 425ms/step - loss: 0.5521 - accuracy: 0.8799\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 406ms/step - loss: 0.5393 - accuracy: 0.8799\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 413ms/step - loss: 0.4668 - accuracy: 0.8917\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s 416ms/step - loss: 0.4352 - accuracy: 0.9016\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14] 12\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 5s 310ms/step - loss: 3.0115 - accuracy: 0.6496\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 4s 403ms/step - loss: 0.8495 - accuracy: 0.8189\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 7s 668ms/step - loss: 0.8458 - accuracy: 0.7933\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 440ms/step - loss: 0.7786 - accuracy: 0.8248\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s 444ms/step - loss: 0.6852 - accuracy: 0.8091\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 468ms/step - loss: 0.6832 - accuracy: 0.8524\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 7s 610ms/step - loss: 0.6076 - accuracy: 0.8445\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 471ms/step - loss: 0.6523 - accuracy: 0.8366\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 485ms/step - loss: 0.5854 - accuracy: 0.8602\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s 434ms/step - loss: 0.5402 - accuracy: 0.8701\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14] 13\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 8s 574ms/step - loss: 4.1943 - accuracy: 0.6988\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 6s 499ms/step - loss: 0.9311 - accuracy: 0.8248\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 431ms/step - loss: 0.9028 - accuracy: 0.8110\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 437ms/step - loss: 0.6678 - accuracy: 0.8307\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s 409ms/step - loss: 0.6895 - accuracy: 0.8268\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 436ms/step - loss: 0.6359 - accuracy: 0.8268\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 425ms/step - loss: 0.6202 - accuracy: 0.8681\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 425ms/step - loss: 0.5510 - accuracy: 0.8661\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 404ms/step - loss: 0.5786 - accuracy: 0.8819\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s 428ms/step - loss: 0.6441 - accuracy: 0.8425\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] 14\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 6s 409ms/step - loss: 2.9943 - accuracy: 0.7205\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 5s 430ms/step - loss: 0.9536 - accuracy: 0.8130\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 5s 458ms/step - loss: 0.7938 - accuracy: 0.8327\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 409ms/step - loss: 0.7716 - accuracy: 0.8760\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 5s 426ms/step - loss: 0.6380 - accuracy: 0.8484\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 5s 436ms/step - loss: 0.5650 - accuracy: 0.8661\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 5s 411ms/step - loss: 0.5512 - accuracy: 0.8661\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 5s 458ms/step - loss: 0.5420 - accuracy: 0.8622\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 5s 415ms/step - loss: 0.5444 - accuracy: 0.8720\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 5s 397ms/step - loss: 0.5297 - accuracy: 0.8858\n"
     ]
    }
   ],
   "source": [
    "groups_set = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17] # ids for subjects in WESAD dataset\n",
    "num_signals = 6 # Number of signals in the WESAD dataset measured by the empatica e4\n",
    "num_output_class = 2 # Number of output classes (2 - non-stress vs stress)\n",
    "num_epochs = 10\n",
    "\n",
    "all_acc_histories = []\n",
    "all_loss_histories = []\n",
    "\n",
    "for i in groups_set:\n",
    "    test_index = groups_set[i]\n",
    "    train_index = [x for x in groups_set if x != test_index]\n",
    "    print(train_index, test_index)\n",
    "\n",
    "    X_train = np.concatenate(np.array([all_subjects_X[x] for x in train_index]))\n",
    "    y_train = np.concatenate(np.array([all_subjects_y[y] for y in train_index]))\n",
    "    X_test = all_subjects_X[test_index]\n",
    "    y_test = all_subjects_y[test_index]\n",
    "\n",
    "    weight_balance = y_train.tolist().count(0)/y_train.tolist().count(1)\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    #tf.keras.backend.clear_session()\n",
    "\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_output_class)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_output_class)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = build_model(num_signals, num_output_class)\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f\"models/wesad_binary_s{subject_ids[test_index]}_{num_epochs}.h5\",  # Path to save the model file\n",
    "        monitor=\"loss\", # The metric name to monitor\n",
    "        save_best_only=True # If True, it only saves the \"best\" model according to the quantity monitored \n",
    "    )\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",     # Quantity to be monitored.\n",
    "        min_delta=0.01,     # Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "        patience=10,        # Number of epochs with no improvement after which training will be stopped.\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    # validation_data=(X_test, y_test),\n",
    "    epochs=num_epochs, \n",
    "    batch_size=50,\n",
    "    verbose=1,\n",
    "    class_weight={0: 1, 1: weight_balance}, # to address the imbalance of the class labels\n",
    "    callbacks = [checkpoint]#, early_stopping]\n",
    ")   \n",
    "    #acc_history = history.history['val_accuracy']\n",
    "    #loss_history = history.history['val_loss']\n",
    "    #all_acc_histories.append(acc_history)\n",
    "    #all_loss_histories.append(loss_history)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0) \n",
    "    # print('Test loss:', round(score[0], 2)) \n",
    "    # print('Test accuracy:', round(score[1], 2))\n",
    "\n",
    "    # test_loss.append(score[0])\n",
    "    # test_acc.append(score[1])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating every models on the corresponding test dataset not seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of CNN model trained on 10 epochs\n",
      "\n",
      "Subject\t\t Accuracy\n",
      "**************************\n",
      "S2\t\t 0.94286\n",
      "S3\t\t 0.71429\n",
      "S4\t\t 0.97222\n",
      "S5\t\t 1.0\n",
      "S6\t\t 0.94444\n",
      "S7\t\t 0.77778\n",
      "S8\t\t 0.91667\n",
      "S9\t\t 0.94444\n",
      "S10\t\t 0.94595\n",
      "S11\t\t 0.78378\n",
      "S13\t\t 1.0\n",
      "S14\t\t 0.51351\n",
      "S15\t\t 1.0\n",
      "S16\t\t 0.91892\n",
      "S17\t\t 0.64865\n",
      "**************************\n",
      "Avg. Accuracy:\t 0.86823\n"
     ]
    }
   ],
   "source": [
    "# Evaluating every models on the corresponding test dataset not seen during training.\n",
    "all_accuracies = []\n",
    "for i, subject_id in enumerate(subject_ids):\n",
    "    X_test = all_subjects_X[i]\n",
    "    y_test = all_subjects_y[i]\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_output_class)\n",
    "    \n",
    "    model_path = f'models/wesad_binary_s{subject_id}_{num_epochs}.h5'\n",
    "\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    accuracy = model.evaluate(X_test, y_test, verbose=0, )[1]\n",
    "    all_accuracies.append(accuracy)\n",
    "\n",
    "print(f'Evaluation of CNN model trained on {num_epochs} epochs\\n')\n",
    "print(f'Subject\\t\\t Accuracy')\n",
    "print(\"**************************\")\n",
    "for i, accuracy in enumerate(all_accuracies):\n",
    "    print(f'S{subject_ids[i]}\\t\\t {round(accuracy, 5)}')\n",
    "\n",
    "print(\"**************************\")\n",
    "print(f'Avg. Accuracy:\\t {round(np.mean(all_accuracies), 5)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bar plot of the accuracy of the different loso models\n",
    "sns.barplot(x=subject_ids, y=all_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./models/wesad_binary_s17_10.h5\"\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "predictions = model.predict(X_test) # make predictions on the test set using the trained model\n",
    "pred_class = np.argmax(predictions, axis=-1) # get the class with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [0.19653362 0.7266721 ]\n",
      "Prediction:  1\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.2987437 0.563043 ]\n",
      "Prediction:  1\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.41746154 0.43661615]\n",
      "Prediction:  1\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.48358414 0.38509127]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.57984984 0.30606028]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.7080584  0.21673487]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.73444486 0.19104524]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.78660244 0.15906236]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.7090117  0.16464692]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.85604537 0.10196819]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.8941529  0.07720542]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.91156244 0.06542813]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.8974828  0.08188976]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.88124657 0.091727  ]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.91203016 0.07367393]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.9252594  0.06408978]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.92874324 0.062387  ]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.9232879  0.06745113]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.9060638  0.07982168]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.9260689  0.06973724]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.94294477 0.0543992 ]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.9302896  0.06424726]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.94172883 0.06040047]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.93663883 0.06522365]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.9375101  0.06541708]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.91759276 0.08515437]\n",
      "Prediction:  0\n",
      "Ground Truth:  0\n",
      "\n",
      "Prediction:  [0.7712602  0.18873487]\n",
      "Prediction:  0\n",
      "Ground Truth:  1\n",
      "\n",
      "Prediction:  [0.7108155  0.22968066]\n",
      "Prediction:  0\n",
      "Ground Truth:  1\n",
      "\n",
      "Prediction:  [0.74587065 0.21776603]\n",
      "Prediction:  0\n",
      "Ground Truth:  1\n",
      "\n",
      "Prediction:  [0.5813141 0.3502618]\n",
      "Prediction:  0\n",
      "Ground Truth:  1\n",
      "\n",
      "Prediction:  [0.7350627  0.24560097]\n",
      "Prediction:  0\n",
      "Ground Truth:  1\n",
      "\n",
      "Prediction:  [0.6562862  0.29769903]\n",
      "Prediction:  0\n",
      "Ground Truth:  1\n",
      "\n",
      "Prediction:  [0.6932381  0.27342346]\n",
      "Prediction:  0\n",
      "Ground Truth:  1\n",
      "\n",
      "Prediction:  [0.597647   0.34326226]\n",
      "Prediction:  0\n",
      "Ground Truth:  1\n",
      "\n",
      "Prediction:  [0.45955706 0.5219192 ]\n",
      "Prediction:  1\n",
      "Ground Truth:  1\n",
      "\n",
      "Prediction:  [0.791276   0.20853475]\n",
      "Prediction:  0\n",
      "Ground Truth:  1\n",
      "\n",
      "Prediction:  [0.8258845 0.1742392]\n",
      "Prediction:  0\n",
      "Ground Truth:  1\n",
      "\n",
      "Accuracy:  0.6486486486486487\n"
     ]
    }
   ],
   "source": [
    "true_classified = 0\n",
    "for i in range(len(pred_class)):\n",
    "    print(\"Prediction: \", predictions[i])\n",
    "    print(\"Prediction: \", pred_class[i])\n",
    "    ground_truth = max(enumerate(y_test[i]),key=lambda x: x[1])[0]\n",
    "    print(\"Ground Truth: \", ground_truth)\n",
    "    print()\n",
    "    if ground_truth == pred_class[i]:\n",
    "        true_classified += 1\n",
    "print(\"Accuracy: \", true_classified/len(pred_class))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
