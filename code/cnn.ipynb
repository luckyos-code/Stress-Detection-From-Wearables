{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import scipy.signal\n",
    "from scipy import fft\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.realpath(\"../data/WESAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject:\n",
    "    \"\"\"Subject of the WESAD dataset.\n",
    "    Subject Class inspired by: https://github.com/WJMatthew/WESAD\"\"\"\n",
    "\n",
    "    def __init__(self, main_path, subject_number):\n",
    "        self.name = f'S{subject_number}'\n",
    "        self.subject_keys = ['signal', 'label', 'subject']\n",
    "        self.signal_keys = ['chest', 'wrist']\n",
    "        self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "        self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "            self.data = pickle.load(file, encoding='latin1')\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        \"\"\"Returns data measured by the E4 Empatica\"\"\"\n",
    "\n",
    "        data = self.data['signal']['wrist']\n",
    "        return data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        \"\"\"Returns data measured by the Respiband wristband\"\"\"\n",
    "        return self.data['signal']['chest']\n",
    "    \n",
    "    def get_subject_dataframe(self):\n",
    "        \"\"\"Returns a dataframe with the preprocessed data of the subject\"\"\"\n",
    "        wrist_data = self.get_wrist_data()\n",
    "        bvp_signal = wrist_data['BVP'][:,0]\n",
    "        eda_signal = wrist_data['EDA'][:,0]\n",
    "        acc_x_signal = wrist_data['ACC'][:,0]\n",
    "        acc_y_signal = wrist_data['ACC'][:,1]\n",
    "        acc_z_signal = wrist_data['ACC'][:,2]\n",
    "        temp_signal = wrist_data['TEMP'][:,0]\n",
    "        # Upsampling data to match BVP data sampling rate using fourier method as described in Paper/dataset\n",
    "        eda_upsampled = scipy.signal.resample(eda_signal, len(bvp_signal))\n",
    "        temp_upsampled = scipy.signal.resample(temp_signal, len(bvp_signal))\n",
    "        acc_x_upsampled = scipy.signal.resample(acc_x_signal, len(bvp_signal))\n",
    "        acc_y_upsampled = scipy.signal.resample(acc_y_signal, len(bvp_signal))\n",
    "        acc_z_upsampled = scipy.signal.resample(acc_z_signal, len(bvp_signal))\n",
    "        label_df = pd.DataFrame(self.labels, columns=['label'])\n",
    "        label_df.index = [(1 / 700) * i for i in range(len(label_df))] # 700 is the sampling rate of the label\n",
    "        label_df.index = pd.to_datetime(label_df.index, unit='s')\n",
    "        data_arrays = zip(bvp_signal, eda_upsampled, acc_x_upsampled, acc_y_upsampled, acc_z_upsampled, temp_upsampled)\n",
    "        df = pd.DataFrame(data=data_arrays, columns=['BVP', 'EDA', 'ACC_x', 'ACC_y', 'ACC_z', 'TEMP'])\n",
    "        df.index = [(1 / 64) * i for i in range(len(df))] # 64 = sampling rate of BVP\n",
    "        df.index = pd.to_datetime(df.index, unit='s')\n",
    "        df = df.join(label_df)\n",
    "        df['label'] = df['label'].fillna(method='ffill')\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.drop(df[df['label'].isin([0.0, 4.0, 5.0, 6.0, 7.0])].index, inplace=True)\n",
    "        df['label'] = df['label'].replace([1.0, 2.0, 3.0], [0, 1, 0])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df = (df-df.min())/(df.max()-df.min()) # Normalize data (no train test leakage since data frame per subject)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with all the subjects and belonging dataframes\n",
    "subjects = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "subjects_data = {}\n",
    "for subject_num in subjects:\n",
    "    subject = Subject(DATA_PATH, subject_num)\n",
    "    subjects_data[subject.name] = subject.get_subject_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Anwendung von Fourier Transformation (scipy.fft) am Beispiel: https://realpython.com/python-scipy-fft/\n",
    "Wichtige Parameter:\n",
    "1. **SAMPLE_RATE** (in Hertz) determines how many data points the signal uses to represent the sine wave per second. So if the signal had a sample rate of 10 Hz and was a five-second sine wave, then it would have 10 * 5 = 50 data points.\n",
    "2. **DURATION** (in Seconds) is the length of the generated sample. -> Frage hierzu: können wir überhaupt duration in seconds nehmen, wenn wir gar nicht mit den Zeitstempeln arbeiten, oder ist das irrelvant?\n",
    "\n",
    "Für *fftfreq* wird N (= SAMPLE_RATE * DURATION) und 1/SAMPLE_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subwindow length of the biosignals\n",
    "signal_subwindow_dict = {\n",
    "    'ACC_x': 7,\n",
    "    'ACC_y': 7,\n",
    "    'ACC_z': 7,\n",
    "    'BVP': 30,\n",
    "    'EDA': 30,\n",
    "    'TEMP': 35\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent element in list\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(df: pd.DataFrame) -> tuple[pd.DataFrame,list]:\n",
    "    \"\"\"Creates windows from the dataframe and returns the windows and the labels.\n",
    "    If the window is assigned to multiple labels, the most common label is chosen for that period.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Subject DataFrame\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame,list]: Windows representing the activity of the subject in one minute and the corresponding labels.\n",
    "    \"\"\"\n",
    "\n",
    "    window_len = 64 * 60 # fs = 64 and window length in seconds = 60\n",
    "    windows, labels = zip(*[(df[i:i+window_len], int(most_common(df['label'][i:i+window_len].to_list()))) for i in range(0,df.shape[0],window_len)])\n",
    "    return windows, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subwindows(df: pd.DataFrame, signal_subwindow_len: int, signal_name: str) -> list:\n",
    "    \"\"\"The function creates subwindows from the windows.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Windows representing the activity of the subject in one minute.\n",
    "        signal_subwindow_len (int): Length of the subwindows.\n",
    "        signal_name (str): Name of the signal.\n",
    "\n",
    "    Returns:\n",
    "        list: Subwindows of the signal in the window.\n",
    "    \"\"\"\n",
    "    subwindow_len = 64 * signal_subwindow_len # fs = 64 and sub-window length in seconds = 30\n",
    "    window_len = 64 * 60 # fs = 64 and window length in seconds = 60\n",
    "    window_shift = int(64 * 0.25) # fs = 64 and window shift in seconds = 0.25\n",
    "    subwindows = []\n",
    "\n",
    "    for i in range(0, window_len, window_shift):\n",
    "        if i + subwindow_len <= window_len:\n",
    "            subwindow = df[signal_name][i:i+subwindow_len]\n",
    "            subwindows.append(subwindow)\n",
    "    return subwindows\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_subwindows(subwindows: list, duration: int, f_s: int) -> list:\n",
    "    \"\"\"Calculates the fft of the subwindows.\n",
    "\n",
    "    Args:\n",
    "        subwindows (list): C\n",
    "        duration (int): Duration of specific signal.\n",
    "        f_s (int): Frequency of the signal.\n",
    "\n",
    "    Returns:\n",
    "        list: Fft coefficients of the subwindows.\n",
    "    \"\"\"\n",
    "    freqs= []\n",
    "    yfs = []\n",
    "    for subwindow in subwindows:\n",
    "        y = np.array(subwindow)\n",
    "        yf = scipy.fft.fft(y)\n",
    "        N = f_s * duration\n",
    "        freq = scipy.fft.fftfreq(N, 1/f_s)\n",
    "        freqs.append(freq)\n",
    "        yfs.append(yf)\n",
    "    return freqs, yfs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_subwindows(subwindows: list, duration: int, f_s: int) -> list:\n",
    "    \"\"\"Calculates the fft of the subwindows.\n",
    "\n",
    "    Args:\n",
    "        subwindows (list): C\n",
    "        duration (int): _description_\n",
    "        f_s (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        list: Fft coefficients of the subwindows.\n",
    "    \"\"\"\n",
    "    freqs= []\n",
    "    yfs = []\n",
    "    for subwindow in subwindows:\n",
    "        y = np.array(subwindow)\n",
    "        yf = scipy.fft.fft(y)\n",
    "        l = len(yf)\n",
    "        N = f_s * duration\n",
    "        freq = scipy.fft.fftfreq(N, 1/f_s)\n",
    "\n",
    "        l //= 2\n",
    "        amps = np.abs(yf[0:l])\n",
    "        freq = np.abs(freq[0:l])\n",
    "\n",
    "        # Sort descending amp   \n",
    "        p = amps.argsort()[::-1]\n",
    "        freq = freq[p]\n",
    "        amps = amps[p]\n",
    "\n",
    "        freqs.append(freq)\n",
    "        yfs.append(amps)\n",
    "    return np.asarray(freqs), np.asarray(yfs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_window(subwindows_fft: list) -> list:\n",
    "    \"\"\"Calculates the average of the fft coefficients of the subwindows.\n",
    "\n",
    "    Args:\n",
    "        subwindows_fft (list): List of fft coefficients of the subwindows.\n",
    "\n",
    "    Returns:\n",
    "        list: Average of the fft coefficients of the subwindow for signals.\n",
    "    \"\"\"\n",
    "    len_yfs = len(subwindows_fft[0])\n",
    "    avg_yfs = []\n",
    "    for i in range(len_yfs):\n",
    "        i_yfs = []\n",
    "        for yf in subwindows_fft:\n",
    "            try:\n",
    "                i_yfs.append(yf[i])\n",
    "            except IndexError:\n",
    "                pass\n",
    "        avg_yfs.append(sum(i_yfs)/len(i_yfs))\n",
    "    return avg_yfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates averaged windows for all subjects from dataframes\n",
    "\n",
    "subjects_preprosessed_data = {}\n",
    "for subject_name, subject_df in subjects_data.items():\n",
    "    subjects_preprosessed_data[subject_name] = {}\n",
    "    windows, labels = create_windows(subject_df)\n",
    "    yfs_per_min_for_signal = {}\n",
    "    X = []\n",
    "    for i in range(0,len(windows) - 1):\n",
    "        for signal in signal_subwindow_dict.keys():\n",
    "\n",
    "            duration_in_sec = signal_subwindow_dict[signal]\n",
    "\n",
    "            subwindows = create_subwindows(windows[i], signal_subwindow_len=duration_in_sec, signal_name=signal)\n",
    "            freqs, yfs = fft_subwindows(subwindows, duration_in_sec, 64)\n",
    "            yfs_average = average_window(yfs)[:210]\n",
    "            yfs_per_min_for_signal[signal] = yfs_average\n",
    "            \n",
    "        X.append(pd.DataFrame(yfs_per_min_for_signal).T)\n",
    "    y = list(labels[:len(windows)-1])\n",
    "    subjects_preprosessed_data[subject_name]['X'] = X\n",
    "    subjects_preprosessed_data[subject_name]['y'] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created train and test data for leave one out cross validation\n",
    "all_subjects_X = []\n",
    "all_subjects_y = []\n",
    "for subject_name, subject_data in subjects_preprosessed_data.items():\n",
    "    all_subjects_X.append(subject_data['X'])\n",
    "    all_subjects_y.append(subject_data['y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_signals: int, num_output_class: int) -> tf.keras.models.Sequential:\n",
    "    # Define the model architecture\n",
    "    model = tf.keras.Sequential()\n",
    "    # input_shape = 14 Signale (bei uns max. 6) X 210 Inputs (aus Tabelle nach Fourier)\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[num_signals, 210, 1]))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, activation='relu', kernel_size=(1,3), strides=1, padding='same')) \n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, activation='relu', kernel_size=(1,3), strides=1, padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(1,2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, activation='relu', kernel_size=(1,3), strides=1, padding='same'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(1,2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=128, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Dense(units=64, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    # Anzahl der Units = Anzahl der Klassen (2 - non-stress vs stress)\n",
    "    model.add(tf.keras.layers.Dense(units=num_output_class, activation='sigmoid', kernel_initializer='glorot_uniform')) # sigmoid statt softmax, da nur 2 Klassen\n",
    "\n",
    "    model.compile(optimizer=\"rmsprop\",loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_set = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17] # ids for subjects in WESAD dataset\n",
    "num_signals = 6 # Number of signals in the WESAD dataset measured by the empatica e4\n",
    "num_output_class = 2 # Number of output classes (2 - non-stress vs stress)\n",
    "num_epochs = 10\n",
    "\n",
    "all_acc_histories = []\n",
    "all_loss_histories = []\n",
    "\n",
    "for i in groups_set:\n",
    "    test_index = groups_set[i]\n",
    "    train_index = [x for x in groups_set if x != test_index]\n",
    "    print(train_index, test_index)\n",
    "\n",
    "    X_train = np.concatenate(np.array([all_subjects_X[x] for x in train_index]))\n",
    "    y_train = np.concatenate(np.array([all_subjects_y[y] for y in train_index]))\n",
    "    X_test = all_subjects_X[test_index]\n",
    "    y_test = all_subjects_y[test_index]\n",
    "\n",
    "    weight_balance = y_train.tolist().count(0)/y_train.tolist().count(1)\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    #tf.keras.backend.clear_session()\n",
    "\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_output_class)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_output_class)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = build_model(num_signals, num_output_class)\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f\"models/wesad_binary_s{subject_ids[test_index]}_{num_epochs}.h5\",  # Path to save the model file\n",
    "        monitor=\"loss\", # The metric name to monitor\n",
    "        save_best_only=True # If True, it only saves the \"best\" model according to the quantity monitored \n",
    "    )\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",     # Quantity to be monitored.\n",
    "        min_delta=0.01,     # Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "        patience=10,        # Number of epochs with no improvement after which training will be stopped.\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    # validation_data=(X_test, y_test),\n",
    "    epochs=num_epochs, \n",
    "    batch_size=50,\n",
    "    verbose=1,\n",
    "    class_weight={0: 1, 1: weight_balance}, # to address the imbalance of the class labels\n",
    "    callbacks = [checkpoint]#, early_stopping]\n",
    ")   \n",
    "    #acc_history = history.history['val_accuracy']\n",
    "    #loss_history = history.history['val_loss']\n",
    "    #all_acc_histories.append(acc_history)\n",
    "    #all_loss_histories.append(loss_history)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0) \n",
    "    # print('Test loss:', round(score[0], 2)) \n",
    "    # print('Test accuracy:', round(score[1], 2))\n",
    "\n",
    "    # test_loss.append(score[0])\n",
    "    # test_acc.append(score[1])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating every models on the corresponding test dataset not seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating every models on the corresponding test dataset not seen during training.\n",
    "all_accuracies = []\n",
    "for i, subject_id in enumerate(subject_ids):\n",
    "    X_test = all_subjects_X[i]\n",
    "    y_test = all_subjects_y[i]\n",
    "    X_test = np.asarray(X_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_output_class)\n",
    "    \n",
    "    model_path = f'models/wesad_binary_s{subject_id}_{num_epochs}.h5'\n",
    "\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    accuracy = model.evaluate(X_test, y_test, verbose=0, )[1]\n",
    "    all_accuracies.append(accuracy)\n",
    "\n",
    "print(f'Evaluation of CNN model trained on {num_epochs} epochs\\n')\n",
    "print(f'Subject\\t\\t Accuracy')\n",
    "print(\"**************************\")\n",
    "for i, accuracy in enumerate(all_accuracies):\n",
    "    print(f'S{subject_ids[i]}\\t\\t {round(accuracy, 5)}')\n",
    "\n",
    "print(\"**************************\")\n",
    "print(f'Avg. Accuracy:\\t {round(np.mean(all_accuracies), 5)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of the accuracy of the different loso models\n",
    "sns.barplot(x=subject_ids, y=all_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/wesad_binary_s16_10.h5\"\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "predictions = model.predict(X_test) # make predictions on the test set using the trained model\n",
    "pred_class = np.argmax(predictions, axis=-1) # get the class with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classified = 0\n",
    "for i in range(len(pred_class)):\n",
    "    print(\"Prediction: \", predictions[i])\n",
    "    print(\"Prediction: \", pred_class[i])\n",
    "    ground_truth = max(enumerate(y_test[i]),key=lambda x: x[1])[0]\n",
    "    print(\"Ground Truth: \", ground_truth)\n",
    "    print()\n",
    "    if ground_truth == pred_class[i]:\n",
    "        true_classified += 1\n",
    "print(\"Accuracy: \", true_classified/len(pred_class))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('sdfw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "421d3e5045432d1c81a7cbcf776867c261bbaddc029f3e12ea9459c25f8592c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
